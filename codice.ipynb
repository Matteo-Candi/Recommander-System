{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, KNNBasic\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModCloth = pd.read_csv('data/ModCloth_clear.csv')\n",
    "\n",
    "# Consideriamo come user ID la variabile user_name poichè presenta meno valori unici nel dataset.\n",
    "len(ModCloth.user_id.unique()) # --> 47176\n",
    "len(ModCloth.user_name.unique()) # --> 31883\n",
    "df = ModCloth[['user_name', 'item_id', 'quality']]\n",
    "df.columns = ['user_id', 'item_id', 'rating']\n",
    "\n",
    "\n",
    "# Estrazione utenti con più di 100 recensioni.\n",
    "best_users = df.groupby('user_id').count().item_id[df.groupby('user_id').count().item_id > 100].index\n",
    "df_sample = df[df.user_id.isin(best_users)]\n",
    "\n",
    "\n",
    "# Modelliamo il dataset per renderlo utilizzabile per allenare il nostro modello (K-NN).\n",
    "reader = Reader(rating_scale=(df_sample.rating.min(), df.rating.max()))\n",
    "data = Dataset.load_from_df(df_sample, reader)\n",
    "\n",
    "\n",
    "# Settiamo i valori degli iperparametri che vogliamo testare.\n",
    "similarity = ['msd', 'cosine', 'pearson', 'pearson_baseline']\n",
    "k_grid = np.arange(1,30) \n",
    "user_based = [False, True]\n",
    "\n",
    "res = {}\n",
    "\n",
    "for based in user_based:\n",
    "    res = {}\n",
    "    for pos,s in tqdm(enumerate(similarity)):\n",
    "        RMSE = []\n",
    "        MSE = []\n",
    "        for k in k_grid:\n",
    "            sim_options = {\n",
    "                \"k\": k,\n",
    "                \"name\": s,\n",
    "                \"user_based\": based, \n",
    "            }\n",
    "\n",
    "            # Per ogni combinazione valutiamo il modello.\n",
    "            model = KNNBasic(sim_options = sim_options)\n",
    "            results = cross_validate(model, data, measures=['RMSE', 'MSE'], cv=3, verbose=0, n_jobs=-1)\n",
    "            RMSE.append(np.mean(results['test_rmse']))\n",
    "            MSE.append(np.mean(results['test_mse']))\n",
    "            \n",
    "        res[s] = (k_grid[np.argmin(RMSE)], min(MSE), min(RMSE))\n",
    "\n",
    "    results = pd.DataFrame(res, index=['k', 'MSE', 'RMSE'])\n",
    "    display(results.style.set_caption(f\"user_based = {based}\"))\n",
    "    \n",
    "    # Salviamo i valori che ottimizzano meglio il modello.\n",
    "    new_k = int(results.loc['k'][np.argmin(results.loc['RMSE'])])\n",
    "    new_metric = results.columns[np.argmin(results.loc['RMSE'])]\n",
    "    new_based = based\n",
    "    new_best_rmse = results[new_metric].loc['RMSE']\n",
    "    new_best_mse = results[new_metric].loc['MSE']\n",
    "    if based:\n",
    "        if new_best_rmse < best_rmse_knn:\n",
    "            best_k = new_k\n",
    "            best_metric = new_metric\n",
    "            best_based = new_based\n",
    "            best_rmse_knn = new_best_rmse\n",
    "            best_mse_knn = new_best_mse\n",
    "    else:\n",
    "        best_k = new_k\n",
    "        best_metric = new_metric\n",
    "        best_based = new_based\n",
    "        best_rmse_knn = new_best_rmse\n",
    "        best_mse_knn = new_best_mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estraiamo gli ID di tutti gli utenti e di tutti i prodotti.\n",
    "unique_users = df_sample.user_id.unique()\n",
    "unique_users.sort()\n",
    "unique_items = df_sample.item_id.unique()\n",
    "unique_items.sort()\n",
    "\n",
    "# Alleniamo il modello con i valori ottimali trovati in precedenza.\n",
    "trainset = data.build_full_trainset()\n",
    "sim_options = {\n",
    "    \"k\": best_k,\n",
    "    \"name\": best_metric,\n",
    "    \"user_based\": best_based, \n",
    "}\n",
    "model = KNNBasic(sim_options = sim_options) \n",
    "model.fit(trainset)\n",
    "\n",
    "# Chiediamo al modello di prevedere la valutazione di ogni utente ad ogni item.\n",
    "test_set = [[user, item, 3] for user in tqdm(unique_users) for item in unique_items]\n",
    "pred = [model.predict(i[0], i[1]).est for i in tqdm(test_set)]\n",
    "\n",
    "# Riempiamo la matrice di rating con i valori predetti.\n",
    "rating_matrix = np.reshape(pred, (len(unique_users), len(unique_items)))\n",
    "rating_dataframe = pd.DataFrame(rating_matrix, columns=unique_items, index=unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo delle cosine similarity tra gli utenti.\n",
    "df_cosine=pd.DataFrame(cosine_similarity(rating_dataframe, dense_output=True))\n",
    "\n",
    "# Elbow Method.\n",
    "cs = []\n",
    "range_k = list(range(1,7))\n",
    "for i in range_k:\n",
    "    kmeans = KMeans(n_clusters = i, max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(df_cosine)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    \n",
    "\n",
    "# Prime 2 componenti principali.\n",
    "data = df_cosine\n",
    "pca = PCA(2)\n",
    "transform = pca.fit_transform(data)\n",
    "\n",
    "# KMeans con valore di k precedentemente trovato.\n",
    "k = 2\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "label = kmeans.fit_predict(transform)\n",
    "u_labels = np.unique(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_recommendation(user_id, ratings_df, n_items, all_users = False):\n",
    "\n",
    "    # Controllo se l'utente esiste nella nostra lista.\n",
    "    users_id = ratings_df[\"user_id\"].unique()\n",
    "    if user_id not in users_id:\n",
    "        return('This users do not exist')\n",
    "\n",
    "    # Consideriamo solo item non recensiti dall'utente.\n",
    "    items_id = ratings_df[\"item_id\"].unique()\n",
    "    item_ids_user = ratings_df.loc[df[\"user_id\"] == user_id, \"item_id\"]\n",
    "    item_ids_to_pred = np.setdiff1d(items_id, item_ids_user)\n",
    "\n",
    "    # Estraiamo i valori predetti.\n",
    "    test_set = [[user_id, item, 3] for item in item_ids_to_pred]\n",
    "    predictions = model.test(test_set)\n",
    "    pred_ratings = np.array([pred.est for pred in predictions])\n",
    "\n",
    "    # Riportiamo i top n item con miglior rating predetto.\n",
    "    index_max = (-pred_ratings).argsort()[:n_items]\n",
    "    \n",
    "    if not all_users:\n",
    "        print(\"Top {0} item recommendations for user {1}:\".format(n_items, user_id))\n",
    "        for i in index_max:\n",
    "            item_id = items_id[i]\n",
    "            print(f'Item: {item_id} rating: {pred_ratings[i]}')\n",
    "    else:\n",
    "        return [items_id[i] for i in index_max]\n",
    "\n",
    "\n",
    "\n",
    "n_items = 5\n",
    "\n",
    "list_n_item = [top_n_recommendation(user, df_sample, n_items, all_users=True) for user in tqdm(unique_users)];\n",
    "recommended_matrix = np.reshape(list_n_item, (len(unique_users), n_items))\n",
    "rating_dataframe = pd.DataFrame(recommended_matrix, columns=list(range(0,n_items)), index=unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adattiamo i nostri dati in modo da renderli utilizzabili per il modello (SVD).\n",
    "reader = Reader(rating_scale=(df_sample.rating.min(), df.rating.max()))\n",
    "data = Dataset.load_from_df(df_sample, reader)\n",
    "\n",
    "\n",
    "grid = {'n_factors':[50, 100, 150],\n",
    "        'n_epochs': [5, 10, 20, 30], \n",
    "        'lr_all': [.0025, .005, .001, .01],\n",
    "        'reg_all':[0.02,0.1]}\n",
    "\n",
    "gs = GridSearchCV(SVD, grid, measures=['RMSE', 'MSE'], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "best_n_factors = gs.best_params['rmse']['n_factors']\n",
    "best_n_epochs = gs.best_params['rmse']['n_epochs']\n",
    "best_lr = gs.best_params['rmse']['lr_all']\n",
    "best_reg = gs.best_params['rmse']['reg_all']\n",
    "\n",
    "best_rmse_mf = gs.best_score['rmse']\n",
    "best_mse_mf = gs.best_score['mse']\n",
    "\n",
    "\n",
    "\n",
    "unique_users = df_sample.user_id.unique()\n",
    "unique_users.sort()\n",
    "unique_items = df_sample.item_id.unique()\n",
    "unique_items.sort()\n",
    "\n",
    "# Training del modello.\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "model = SVD(n_factors=best_n_factors, n_epochs=best_n_epochs, lr_all=best_lr, reg_all=best_reg)\n",
    "model.fit(trainset)\n",
    "\n",
    "test_set = [(user, item) for user in unique_users for item in unique_items]\n",
    "pred = [model.predict(i[0], i[1]).est for i in test_set]\n",
    "\n",
    "rating_matrix = np.reshape(pred, (len(unique_users), len(unique_items)))\n",
    "rating_dataframe = pd.DataFrame(rating_matrix, columns=unique_items, index=unique_users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "773286f42bdafd8588749582ed653d25b280a530e585f5f155cb0082e5f8cdaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
